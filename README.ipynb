{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc0b453",
   "metadata": {},
   "source": [
    "# Linear Regression train by Gradient Descent and Visualize It\n",
    "## Description\n",
    "* Use Python Numpy and matplotlib, simply implement Linear Regression and show training weight trace in each step <br>also visualize with contour figure.\n",
    "## Implement Details\n",
    "* We need to use a function  $ Y=W \\times X + B $ to fit Features(x_data) and Target(y_data)\n",
    "* Here we use gradient descent to update ours weights\n",
    "* Loss Function: $MSE=\\sum\\limits_{i=1}^{n}(y_i-\\hat{y}_i)^2$ \n",
    "* Gradient: Calculate partial differential of Loss Fucntion\n",
    "    * Let $ L = \\sum\\limits_{i=1}^{n}(y_i-\\hat{y}_i)^2 = \\sum\\limits_{i=1}^{n}(y_i-(wx+b))^2 $\n",
    "    * $ \\frac{\\partial L}{\\partial w} = \\sum\\limits_{i=1}^{n}2x_i(b_i+w_ix_i-y_i) $\n",
    "    * $\\frac{\\partial L}{\\partial b} = \\sum\\limits_{i=1}^{n}2(b_i+w_ix_i-y_i)$\n",
    "\n",
    "\n",
    "## Code OutLook:\n",
    "```python\n",
    "# initial weight and bias\n",
    "b=-120\n",
    "w=-4\n",
    "# base learning rate\n",
    "lr=1.3\n",
    "# iteration\n",
    "iteration=50000\n",
    "# store each weight and bias to plot contour figure\n",
    "b_history=[b]\n",
    "w_history=[w]\n",
    "# Here is the trick for AdaGrad(Adavance Learning Rate Trick)\n",
    "lr_b=0.0\n",
    "lr_w=0.0\n",
    "for i in range(iteration):\n",
    "    # zero gradient first\n",
    "    b_grad=0.0  \n",
    "    w_grad=0.0   \n",
    "    # compute all data\n",
    "    for n in range(len(x_data)):\n",
    "        # the partial differential of L(w,b) with respect to b\n",
    "        b_grad = b_grad -2.0*(y_data[n] - b - w*x_data[n] )*1.0\n",
    "        # the partial differential of L(w,b) with respect to weight\n",
    "        w_grad = w_grad -2.0*(y_data[n] - b - w*x_data[n] )*x_data[n] \n",
    "    # Bellow is the trick for AdaGrad(Adavance Learning Rate Trick)\n",
    "    lr_b = lr_b + b_grad **2\n",
    "    lr_w = lr_w + w_grad **2\n",
    "    b = b - lr/np.sqrt(lr_b)*b_grad # use Adagrad\n",
    "    w = w - lr/np.sqrt(lr_w)*w_grad\n",
    "    # Above is the trick for AdaGrad(Adavance Learning Rate Trick)\n",
    "    # Below: store each weight and bias\n",
    "    b_history.append(b)\n",
    "    w_history.append(w)\n",
    "```\n",
    "\n",
    "## Result\n",
    "* We can see the training step is getting thicker, and red fork is target(which Loss is the lowest one)<br>\n",
    "<img src=\"https://i.imgur.com/b0YnARZ.png\" width=280 align=\"left\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e14a64",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Gradient}}$\n",
    "***\n",
    "* Initialize Mode: $ y = a + bx $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720303c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
